# ç°ä»£æ•°æ®æ¶æ„ï¼ˆData Lakehouseï¼‰é€‰å‹æŒ‡å—

> ğŸ“… æ›´æ–°æ—¶é—´ï¼š2026-02-16  
> ğŸ“Š ç ”ç©¶èŒƒå›´ï¼šLakehouseã€å®æ—¶æ•°ä»“ã€æ•°æ®æ²»ç†ã€ç‰¹å¾å¹³å°ã€æµæ°´çº¿ç¼–æ’

---

## ç›®å½•

1. [Lakehouseæ¶æ„åŸç†](#1-lakehouseæ¶æ„åŸç†)
2. [å®æ—¶æ•°ä»“æ„å»º](#2-å®æ—¶æ•°ä»“æ„å»º)
3. [æ•°æ®æ²»ç†ä¸æ•°æ®è´¨é‡](#3-æ•°æ®æ²»ç†ä¸æ•°æ®è´¨é‡)
4. [å®æ—¶ç‰¹å¾å¹³å°æ¶æ„](#4-å®æ—¶ç‰¹å¾å¹³å°æ¶æ„)
5. [æ•°æ®æµæ°´çº¿ç¼–æ’](#5-æ•°æ®æµæ°´çº¿ç¼–æ’)
6. [ç»¼åˆæ¶æ„é€‰å‹å»ºè®®](#6-ç»¼åˆæ¶æ„é€‰å‹å»ºè®®)

---

## 1. Lakehouseæ¶æ„åŸç†

### 1.1 ä»€ä¹ˆæ˜¯Lakehouseï¼Ÿ

Lakehouseæ˜¯ä¸€ç§æ–°å‹æ•°æ®æ¶æ„ï¼Œç»“åˆäº†æ•°æ®æ¹–ï¼ˆData Lakeï¼‰å’Œæ•°æ®ä»“åº“ï¼ˆData Warehouseï¼‰çš„ä¼˜åŠ¿ï¼š

| ç‰¹æ€§ | æ•°æ®æ¹– | æ•°æ®ä»“åº“ | Lakehouse |
|------|--------|----------|-----------|
| å­˜å‚¨æˆæœ¬ | ä½ | é«˜ | ä½ |
| å­˜å‚¨æ ¼å¼ | å¼€æ”¾æ ¼å¼ | ä¸“æœ‰æ ¼å¼ | å¼€æ”¾æ ¼å¼ |
| ACIDäº‹åŠ¡ | âŒ | âœ… | âœ… |
| æ•°æ®æ²»ç† | å¼± | å¼º | å¼º |
| æŸ¥è¯¢æ€§èƒ½ | ä¸€èˆ¬ | é«˜ | é«˜ |
| æ”¯æŒML/AI | å¼º | å¼± | å¼º |

### 1.2 æ ¸å¿ƒæŠ€æœ¯å¯¹æ¯”ï¼šIceberg vs Delta Lake

| ç»´åº¦ | Apache Iceberg | Delta Lake |
|------|----------------|------------|
| **å¼€å‘æ–¹** | Netflix/Apple | Databricks |
| **ç‰ˆæœ¬æ—¶é—´** | 2018 | 2019 |
| **è®¡ç®—å¼•æ“** | Spark, Flink, Trino, Hive, Athena | Spark, Flink (æœ‰é™), Redshift Spectrum |
| **å­˜å‚¨æ ¼å¼** | Parquet, ORC, Avro | Parquet |
| **äº‹åŠ¡æ”¯æŒ** | âœ… ACID | âœ… ACID |
| **Time Travel** | âœ… ç‰ˆæœ¬å›æ»š | âœ… ç‰ˆæœ¬å›æ»š |
| **åˆ†åŒºæ¼”è¿›** | âœ… æ— éœ€é‡å†™æ•°æ® | âš ï¸ éœ€è¦ç‰¹å®šæ“ä½œ |
| **éšè—åˆ†åŒº** | âœ… è‡ªåŠ¨å¤„ç† | âš ï¸ æ‰‹åŠ¨ç®¡ç† |
| **å¹¶å‘æ§åˆ¶** | ä¹è§‚é” | ä¹è§‚é” |
| **ç¤¾åŒºæ´»è·ƒåº¦** | â­â­â­â­â­ | â­â­â­â­ |
| **äº‘åŸç”Ÿæ”¯æŒ** | AWS/GCP/Azure/é˜¿é‡Œäº‘ | Databricks/AWS/é˜¿é‡Œäº‘ |

### 1.3 Apache Iceberg æ ¸å¿ƒåŸç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Iceberg æ¶æ„å›¾                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Catalog Layer                         â”‚   â”‚
â”‚  â”‚    (Hive Metastore / Glue / Nessie / JDBC)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Metadata Layer                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  metadata   â”‚â”€â”€â”€â–¶â”‚  metadata   â”‚â”€â”€â”€â–¶â”‚  metadata   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚   v1.json   â”‚    â”‚   v2.json   â”‚    â”‚   v3.json   â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚         â”‚                                              â”‚   â”‚
â”‚  â”‚         â–¼                                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  manifest   â”‚    â”‚  manifest   â”‚    â”‚  manifest   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚   list-1    â”‚    â”‚   list-2    â”‚    â”‚   list-3    â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚         â”‚                                              â”‚   â”‚
â”‚  â”‚         â–¼                                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  manifest   â”‚    â”‚  manifest   â”‚    â”‚  manifest   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚    1.avro   â”‚    â”‚    2.avro   â”‚    â”‚    3.avro   â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Data Layer (Parquet)                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚data-1   â”‚ â”‚data-2   â”‚ â”‚data-3   â”‚ â”‚data-4   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚.parquet â”‚ â”‚.parquet â”‚ â”‚.parquet â”‚ â”‚.parquet â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Iceberg æ ¸å¿ƒç‰¹æ€§è¯¦è§£

**1. å…ƒæ•°æ®åˆ†å±‚ç»“æ„**
```json
{
  "format-version": 2,
  "table-uuid": "fb072c92-a02b-11e9-ae9c-1bb7bc9eca44",
  "location": "s3://bucket/table/",
  "last-updated-ms": 1515100955770,
  "last-column-id": 3,
  "schema": {
    "type": "struct",
    "schema-id": 0,
    "fields": [
      {"id": 1, "name": "id", "type": "long", "required": true},
      {"id": 2, "name": "data", "type": "string", "required": false},
      {"id": 3, "name": "ts", "type": "timestamp", "required": false}
    ]
  },
  "partition-spec": {
    "spec-id": 0,
    "fields": [
      {"id": 1000, "name": "ts_day", "transform": "day", "source-id": 3}
    ]
  },
  "default-spec-id": 0,
  "snapshots": [
    {
      "snapshot-id": 3055729675574597000,
      "timestamp-ms": 1515100955770,
      "summary": {
        "operation": "append",
        "added-files-size": "1024",
        "added-data-files": "1",
        "added-records": "1000"
      },
      "manifest-list": "s3://bucket/table/metadata/snap-3055729...-1.avro"
    }
  ]
}
```

**2. Time Travel æŸ¥è¯¢**
```sql
-- æŸ¥è¯¢å†å²ç‰ˆæœ¬æ•°æ®
SELECT * FROM table_name TIMESTAMP AS OF '2024-01-01 00:00:00';

-- æŸ¥è¯¢ç‰¹å®šå¿«ç…§
SELECT * FROM table_name VERSION AS OF 3055729675574597000;

-- å›æ»šåˆ°å†å²ç‰ˆæœ¬
CALL system.rollback_to_snapshot('table_name', 3055729675574597000);
```

**3. åˆ†åŒºæ¼”è¿›ï¼ˆPartition Evolutionï¼‰**
```sql
-- åˆå§‹åˆ†åŒºï¼šæŒ‰æœˆåˆ†åŒº
CREATE TABLE events (
    id BIGINT,
    event_time TIMESTAMP,
    event_type STRING
) PARTITIONED BY (months(event_time));

-- åˆ†åŒºæ¼”è¿›ï¼šæ”¹ä¸ºæŒ‰å¤©åˆ†åŒºï¼ˆæ— éœ€é‡å†™å†å²æ•°æ®ï¼‰
ALTER TABLE events ADD PARTITION FIELD days(event_time);
```

### 1.4 Delta Lake æ ¸å¿ƒåŸç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Delta Lake æ¶æ„å›¾                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                Delta Log (Transaction Log)              â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  _delta_log/                                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ 00000000000000000000.json  (åˆå§‹ç‰ˆæœ¬)              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ 00000000000000000001.json  (äº‹åŠ¡1)                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ 00000000000000000002.json  (äº‹åŠ¡2)                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ 00000000000000000003.json  (äº‹åŠ¡3)                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ...                                                â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Transaction JSON å†…å®¹ç¤ºä¾‹                  â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  {                                                      â”‚   â”‚
â”‚  â”‚    "commitInfo": {                                      â”‚   â”‚
â”‚  â”‚      "timestamp": 1234567890,                           â”‚   â”‚
â”‚  â”‚      "operation": "WRITE",                              â”‚   â”‚
â”‚  â”‚      "operationParameters": {...}                       â”‚   â”‚
â”‚  â”‚    },                                                   â”‚   â”‚
â”‚  â”‚    "add": [                                             â”‚   â”‚
â”‚  â”‚      {                                                  â”‚   â”‚
â”‚  â”‚        "path": "part-001.parquet",                      â”‚   â”‚
â”‚  â”‚        "size": 1024,                                    â”‚   â”‚
â”‚  â”‚        "partitionValues": {"date": "2024-01-01"},       â”‚   â”‚
â”‚  â”‚        "stats": {...}                                   â”‚   â”‚
â”‚  â”‚      }                                                  â”‚   â”‚
â”‚  â”‚    ],                                                   â”‚   â”‚
â”‚  â”‚    "remove": [...]                                      â”‚   â”‚
â”‚  â”‚  }                                                      â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Data Files (Parquet)                  â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  data/                                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ date=2024-01-01/                                   â”‚   â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ part-001.parquet                               â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ part-002.parquet                               â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ date=2024-01-02/                                   â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€ part-003.parquet                               â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ...                                                â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Delta Lake æ ¸å¿ƒä»£ç ç¤ºä¾‹

**1. åŸºæœ¬æ“ä½œ**
```python
from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession

# é…ç½® Spark + Delta Lake
builder = SparkSession.builder \
    .appName("DeltaLakeApp") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# åˆ›å»º Delta è¡¨
df = spark.range(0, 5)
df.write.format("delta").save("/tmp/delta-table")

# è¯»å– Delta è¡¨
df = spark.read.format("delta").load("/tmp/delta-table")
df.show()
```

**2. åˆå¹¶æ“ä½œï¼ˆUpsertï¼‰**
```python
from delta.tables import DeltaTable

# ç›®æ ‡è¡¨
deltaTable = DeltaTable.forPath(spark, "/tmp/delta-table")

# æºæ•°æ®
source_df = spark.range(3, 8)

# æ‰§è¡Œ Merge æ“ä½œ
deltaTable.alias("target").merge(
    source_df.alias("source"),
    "target.id = source.id"
).whenMatchedUpdate(set={"id": "source.id"}) \
 .whenNotMatchedInsert(values={"id": "source.id"}) \
 .execute()
```

**3. Time Travel**
```python
# æŸ¥çœ‹å†å²ç‰ˆæœ¬
spark.sql("DESCRIBE HISTORY delta.`/tmp/delta-table`").show()

# æŸ¥è¯¢å†å²ç‰ˆæœ¬
df = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .load("/tmp/delta-table")

# æŒ‰æ—¶é—´æˆ³æŸ¥è¯¢
df = spark.read.format("delta") \
    .option("timestampAsOf", "2024-01-01") \
    .load("/tmp/delta-table")
```

### 1.5 Lakehouse é€‰å‹å»ºè®®

| åœºæ™¯ | æ¨èé€‰æ‹© | ç†ç”± |
|------|----------|------|
| å¤šå¼•æ“ç”Ÿæ€ï¼ˆFlink/Trino/Sparkï¼‰ | **Iceberg** | æ›´å¹¿æ³›çš„è®¡ç®—å¼•æ“æ”¯æŒ |
| Databricks ç”Ÿæ€ | **Delta Lake** | åŸç”Ÿé›†æˆï¼ŒåŠŸèƒ½æœ€å…¨ |
| éœ€è¦åˆ†åŒºæ¼”è¿› | **Iceberg** | æ— éœ€é‡å†™æ•°æ® |
| éœ€è¦éšè—åˆ†åŒº | **Iceberg** | è‡ªåŠ¨å¤„ç†ï¼Œå¯¹ç”¨æˆ·é€æ˜ |
| æœºå™¨å­¦ä¹ åœºæ™¯ | **Delta Lake** | MLflow é›†æˆæ›´å¥½ |
| äº‘åŸç”Ÿéƒ¨ç½² | **Iceberg** | äº‘å‚å•†æ”¯æŒæ›´å¹¿æ³› |
| å®æ—¶æµå¤„ç† | **Iceberg** | Flink æ”¯æŒæ›´æˆç†Ÿ |

---

## 2. å®æ—¶æ•°ä»“æ„å»º

### 2.1 Flink + StarRocks æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å®æ—¶æ•°ä»“åˆ†å±‚æ¶æ„ (Flink + StarRocks)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚   Kafka/     â”‚   â”‚   MySQL/     â”‚   â”‚   Log/       â”‚              â”‚
â”‚   â”‚   Pulsar     â”‚   â”‚   PG (CDC)   â”‚   â”‚   Files      â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚          â”‚                  â”‚                  â”‚                       â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                             â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚                    Flink å®æ—¶è®¡ç®—å±‚                       â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚        â”‚
â”‚   â”‚  â”‚ æ•°æ®æ¸…æ´—    â”‚  â”‚ æ•°æ®å…³è”    â”‚  â”‚ çª—å£èšåˆ    â”‚      â”‚        â”‚
â”‚   â”‚  â”‚ ETL        â”‚  â”‚ Join       â”‚  â”‚ Aggregate  â”‚      â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚        â”‚
â”‚   â”‚                           â–¼                              â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚   â”‚  â”‚              Flink SQL / Table API                  â”‚ â”‚        â”‚
â”‚   â”‚  â”‚  - Stream Processing                              â”‚ â”‚        â”‚
â”‚   â”‚  â”‚  - Temporal Join                                  â”‚ â”‚        â”‚
â”‚   â”‚  â”‚  - Window Functions                               â”‚ â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                               â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚                    æ•°æ®å­˜å‚¨å±‚                             â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚        â”‚
â”‚   â”‚  â”‚  Kafka      â”‚  â”‚  Iceberg/   â”‚  â”‚  StarRocks  â”‚      â”‚        â”‚
â”‚   â”‚  â”‚ (æµæ•°æ®)    â”‚  â”‚  HDFS       â”‚  â”‚ (OLAP)      â”‚      â”‚        â”‚
â”‚   â”‚  â”‚             â”‚  â”‚ (æ‰¹æ•°æ®)    â”‚  â”‚             â”‚      â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                               â”‚                                        â”‚
â”‚                               â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚              StarRocks å®æ—¶æ•°ä»“å±‚                         â”‚        â”‚
â”‚   â”‚                                                          â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚        â”‚
â”‚   â”‚  â”‚   ODS       â”‚â”€â”€â–¶â”‚   DWD       â”‚â”€â”€â–¶â”‚   DWS       â”‚      â”‚        â”‚
â”‚   â”‚  â”‚ åŸå§‹æ•°æ®    â”‚  â”‚ æ˜ç»†æ•°æ®    â”‚  â”‚ æ±‡æ€»æ•°æ®    â”‚      â”‚        â”‚
â”‚   â”‚  â”‚ (æ˜ç»†æ¨¡å‹)  â”‚  â”‚ (æ˜ç»†æ¨¡å‹)  â”‚  â”‚ (èšåˆæ¨¡å‹)  â”‚      â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚
â”‚   â”‚         â”‚                  â”‚                  â”‚          â”‚        â”‚
â”‚   â”‚         â–¼                  â–¼                  â–¼          â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚        â”‚
â”‚   â”‚  â”‚  Routine    â”‚  â”‚  Routine    â”‚  â”‚  Materializedâ”‚      â”‚        â”‚
â”‚   â”‚  â”‚  Load       â”‚  â”‚  Load       â”‚  â”‚  View        â”‚      â”‚        â”‚
â”‚   â”‚  â”‚  (æµå¼å¯¼å…¥) â”‚  â”‚  (æµå¼å¯¼å…¥) â”‚  â”‚  (ç‰©åŒ–è§†å›¾)  â”‚      â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚
â”‚   â”‚                                                          â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚   â”‚  â”‚                   ADS (åº”ç”¨æ•°æ®å±‚)                   â”‚ â”‚        â”‚
â”‚   â”‚  â”‚  - å®æ—¶æŠ¥è¡¨                                          â”‚ â”‚        â”‚
â”‚   â”‚  â”‚  - å³å¸­æŸ¥è¯¢                                          â”‚ â”‚        â”‚
â”‚   â”‚  â”‚  - BI Dashboard                                      â”‚ â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                               â”‚                                        â”‚
â”‚                               â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚                   æ•°æ®åº”ç”¨å±‚                              â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚        â”‚
â”‚   â”‚  â”‚ BIå·¥å…·  â”‚ â”‚ æ•°æ®å¤§å±â”‚ â”‚ æŠ¥è¡¨    â”‚ â”‚ APIæœåŠ¡ â”‚        â”‚        â”‚
â”‚   â”‚  â”‚(FineBI) â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚        â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Flink å®æ—¶è®¡ç®—æ ¸å¿ƒä»£ç 

**1. Flink SQL CDC è¯»å– MySQL**
```sql
-- åˆ›å»º MySQL CDC æºè¡¨
CREATE TABLE mysql_orders (
    order_id BIGINT PRIMARY KEY NOT NULL,
    user_id BIGINT,
    product_id BIGINT,
    amount DECIMAL(10,2),
    order_time TIMESTAMP,
    -- CDC å…ƒæ•°æ®åˆ—
    op_ts TIMESTAMP METADATA FROM 'op_ts' VIRTUAL,
    op_type STRING METADATA FROM 'op_type' VIRTUAL
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'mysql-host',
    'port' = '3306',
    'username' = 'flink',
    'password' = 'password',
    'database-name' = 'ecommerce',
    'table-name' = 'orders',
    'server-id' = '5400-5404',
    'scan.startup.mode' = 'initial'
);

-- åˆ›å»º Kafka ç›®æ ‡è¡¨
CREATE TABLE kafka_orders (
    order_id BIGINT,
    user_id BIGINT,
    product_id BIGINT,
    amount DECIMAL(10,2),
    order_time TIMESTAMP,
    op_type STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'orders',
    'properties.bootstrap.servers' = 'kafka:9092',
    'format' = 'json'
);

-- å®æ—¶åŒæ­¥æ•°æ®
INSERT INTO kafka_orders
SELECT order_id, user_id, product_id, amount, order_time, op_type
FROM mysql_orders;
```

**2. Flink çª—å£èšåˆè®¡ç®—**
```sql
-- åˆ›å»º Kafka æºè¡¨
CREATE TABLE kafka_events (
    user_id STRING,
    event_type STRING,
    event_time TIMESTAMP(3),
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'user_events',
    'properties.bootstrap.servers' = 'kafka:9092',
    'format' = 'json',
    'scan.startup.mode' = 'latest-offset'
);

-- å®æ—¶ UV/PV ç»Ÿè®¡ï¼ˆæ»šåŠ¨çª—å£ï¼‰
CREATE TABLE realtime_stats (
    window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
    event_type STRING,
    pv BIGINT,
    uv BIGINT,
    PRIMARY KEY (window_start, window_end, event_type) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://starrocks:9030/analytics',
    'table-name' = 'realtime_stats',
    'username' = 'root',
    'password' = 'password'
);

-- çª—å£èšåˆè®¡ç®—
INSERT INTO realtime_stats
SELECT
    TUMBLE_START(event_time, INTERVAL '1' MINUTE) as window_start,
    TUMBLE_END(event_time, INTERVAL '1' MINUTE) as window_end,
    event_type,
    COUNT(*) as pv,
    COUNT(DISTINCT user_id) as uv
FROM kafka_events
GROUP BY
    TUMBLE(event_time, INTERVAL '1' MINUTE),
    event_type;
```

**3. Flink DataStream API å¤æ‚å¤„ç†**
```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;

public class RealTimeAnalytics {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        
        // é…ç½® Checkpoint
        env.enableCheckpointing(60000);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        
        // è¯»å– Kafka æ•°æ®
        KafkaSource<Order> source = KafkaSource.<Order>builder()
            .setBootstrapServers("kafka:9092")
            .setTopics("orders")
            .setGroupId("flink-order-processor")
            .setStartingOffsets(OffsetsInitializer.latest())
            .setValueOnlyDeserializer(new JsonDeserializationSchema())
            .build();
        
        DataStream<Order> orders = env.fromSource(
            source, 
            WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                .withTimestampAssigner((order, timestamp) -> order.getOrderTime()),
            "Kafka Orders"
        );
        
        // å®æ—¶å…³è”ç»´åº¦æ•°æ® (Async I/O)
        DataStream<EnrichedOrder> enrichedOrders = AsyncDataStream
            .unorderedWait(
                orders,
                new AsyncUserInfoLookup(),
                1000, // è¶…æ—¶æ—¶é—´
                TimeUnit.MILLISECONDS,
                100  // å¹¶å‘æ•°
            );
        
        // çª—å£èšåˆ
        SingleOutputStreamOperator<OrderStats> stats = enrichedOrders
            .keyBy(EnrichedOrder::getCategory)
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .aggregate(new OrderStatsAggregate());
        
        // å†™å…¥ StarRocks
        stats.addSink(new StarRocksSink<>());
        
        env.execute("Real-time Order Analytics");
    }
}
```

### 2.3 StarRocks å®æ—¶æ•°ä»“è®¾è®¡

**1. æ•°æ®æ¨¡å‹é€‰æ‹©**

| æ¨¡å‹ | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ |
|------|----------|------|
| **æ˜ç»†æ¨¡å‹** | ODS/DWD å±‚ | ä¿ç•™å…¨é‡æ˜ç»†æ•°æ®ï¼Œæ”¯æŒä»»æ„ç»´åº¦åˆ†æ |
| **èšåˆæ¨¡å‹** | DWS/ADS å±‚ | é¢„èšåˆï¼ŒæŸ¥è¯¢æ€§èƒ½é«˜ï¼Œé€‚åˆå›ºå®šç»´åº¦åˆ†æ |
| **æ›´æ–°æ¨¡å‹** | æ‹‰é“¾è¡¨/æœ€æ–°çŠ¶æ€ | æ”¯æŒä¸»é”®æ›´æ–°ï¼Œé€‚åˆç”¨æˆ·ç”»åƒç­‰ |
| **ä¸»é”®æ¨¡å‹** | å®æ—¶æ›´æ–°åœºæ™¯ | æ”¯æŒå¢åˆ æ”¹æŸ¥ï¼Œé€‚åˆ CDC æ•°æ® |

**2. StarRocks å»ºè¡¨ç¤ºä¾‹**
```sql
-- ODS å±‚ï¼šæ˜ç»†æ¨¡å‹
CREATE TABLE ods_orders (
    order_id BIGINT COMMENT 'è®¢å•ID',
    user_id BIGINT COMMENT 'ç”¨æˆ·ID',
    product_id BIGINT COMMENT 'å•†å“ID',
    amount DECIMAL(18,2) COMMENT 'è®¢å•é‡‘é¢',
    order_status INT COMMENT 'è®¢å•çŠ¶æ€',
    create_time DATETIME COMMENT 'åˆ›å»ºæ—¶é—´',
    dt DATE COMMENT 'åˆ†åŒºæ—¥æœŸ'
) ENGINE = OLAP
DUPLICATE KEY(order_id)
COMMENT 'è®¢å•æ˜ç»†è¡¨'
PARTITION BY RANGE(dt) (
    START ("2024-01-01") END ("2025-01-01") EVERY (INTERVAL 1 MONTH)
)
DISTRIBUTED BY HASH(order_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "storage_format" = "DEFAULT"
);

-- DWD å±‚ï¼šå…³è”åçš„æ˜ç»†æ•°æ®
CREATE TABLE dwd_order_detail (
    order_id BIGINT COMMENT 'è®¢å•ID',
    user_id BIGINT COMMENT 'ç”¨æˆ·ID',
    user_name VARCHAR(64) COMMENT 'ç”¨æˆ·åç§°',
    product_id BIGINT COMMENT 'å•†å“ID',
    product_name VARCHAR(128) COMMENT 'å•†å“åç§°',
    category_id INT COMMENT 'å“ç±»ID',
    category_name VARCHAR(64) COMMENT 'å“ç±»åç§°',
    amount DECIMAL(18,2) COMMENT 'è®¢å•é‡‘é¢',
    create_time DATETIME COMMENT 'åˆ›å»ºæ—¶é—´',
    dt DATE COMMENT 'åˆ†åŒºæ—¥æœŸ'
) ENGINE = OLAP
DUPLICATE KEY(order_id)
COMMENT 'è®¢å•æ˜ç»†å®½è¡¨'
PARTITION BY RANGE(dt) (
    START ("2024-01-01") END ("2025-01-01") EVERY (INTERVAL 1 MONTH)
)
DISTRIBUTED BY HASH(order_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3"
);

-- DWS å±‚ï¼šèšåˆæ¨¡å‹
CREATE TABLE dws_order_stats (
    dt DATE COMMENT 'æ—¥æœŸ',
    category_id INT COMMENT 'å“ç±»ID',
    category_name VARCHAR(64) COMMENT 'å“ç±»åç§°',
    total_orders BIGINT SUM COMMENT 'è®¢å•æ€»æ•°',
    total_amount DECIMAL(18,2) SUM COMMENT 'è®¢å•æ€»é‡‘é¢',
    unique_users BIGINT DISTINCT_COUNT COMMENT 'ç‹¬ç«‹ç”¨æˆ·æ•°'
) ENGINE = OLAP
AGGREGATE KEY(dt, category_id, category_name)
COMMENT 'è®¢å•æ—¥ç»Ÿè®¡'
PARTITION BY RANGE(dt) (
    START ("2024-01-01") END ("2025-01-01") EVERY (INTERVAL 1 MONTH)
)
DISTRIBUTED BY HASH(category_id) BUCKETS 8
PROPERTIES (
    "replication_num" = "3"
);

-- ADS å±‚ï¼šä¸»é”®æ¨¡å‹ï¼ˆæ”¯æŒå®æ—¶æ›´æ–°ï¼‰
CREATE TABLE ads_user_realtime (
    user_id BIGINT PRIMARY KEY COMMENT 'ç”¨æˆ·ID',
    user_name VARCHAR(64) COMMENT 'ç”¨æˆ·å',
    total_orders BIGINT COMMENT 'ç´¯è®¡è®¢å•æ•°',
    total_amount DECIMAL(18,2) COMMENT 'ç´¯è®¡æ¶ˆè´¹é‡‘é¢',
    last_order_time DATETIME COMMENT 'æœ€åä¸‹å•æ—¶é—´',
    user_level INT COMMENT 'ç”¨æˆ·ç­‰çº§',
    update_time DATETIME COMMENT 'æ›´æ–°æ—¶é—´'
) ENGINE = OLAP
PRIMARY KEY(user_id)
COMMENT 'ç”¨æˆ·å®æ—¶ç”»åƒ'
DISTRIBUTED BY HASH(user_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "enable_persistent_index" = "true"
);
```

**3. å®æ—¶æ•°æ®å¯¼å…¥**
```sql
-- åˆ›å»º Routine Load ä»»åŠ¡
CREATE ROUTINE LOAD db1.user_orders_load ON ods_orders
COLUMNS TERMINATED BY ',',
COLUMNS(order_id, user_id, product_id, amount, order_status, create_time, dt)
PROPERTIES (
    "desired_concurrent_number" = "3",
    "max_batch_interval" = "20",
    "max_batch_rows" = "300000",
    "max_batch_size" = "209715200"
)
FROM KAFKA (
    "kafka_broker_list" = "kafka1:9092,kafka2:9092",
    "kafka_topic" = "orders",
    "kafka_partitions" = "0,1,2,3",
    "kafka_offsets" = "OFFSET_BEGINNING"
);

-- åˆ›å»ºç‰©åŒ–è§†å›¾å®ç°è‡ªåŠ¨èšåˆ
CREATE MATERIALIZED VIEW dws_order_stats_mv AS
SELECT 
    dt,
    category_id,
    category_name,
    COUNT(*) as total_orders,
    SUM(amount) as total_amount,
    COUNT(DISTINCT user_id) as unique_users
FROM dwd_order_detail
GROUP BY dt, category_id, category_name;
```

### 2.4 å®æ—¶æ•°ä»“é€‰å‹å»ºè®®

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | è¯´æ˜ |
|------|----------|------|
| çº¯å®æ—¶åœºæ™¯ | Flink + StarRocks | æ¯«ç§’çº§å»¶è¿Ÿï¼Œé«˜æ€§èƒ½ |
| å®æ—¶+ç¦»çº¿æ··åˆ | Flink + Iceberg + StarRocks | æµæ‰¹ä¸€ä½“ï¼Œæ•°æ®ä¸€è‡´ |
| è¶…å¤§è§„æ¨¡æ•°æ® | Flink + ClickHouse | ClickHouse æ›´é€‚åˆæµ·é‡æ•°æ® |
| äº‘åŸç”Ÿéƒ¨ç½² | Flink + BigQuery/Redshift | æ‰˜ç®¡æœåŠ¡ï¼Œè¿ç»´ç®€å• |
| æˆæœ¬æ§åˆ¶ | Flink + Trino + S3 | å­˜ç®—åˆ†ç¦»ï¼Œæˆæœ¬æœ€ä¼˜ |

---

## 3. æ•°æ®æ²»ç†ä¸æ•°æ®è´¨é‡

### 3.1 æ•°æ®æ²»ç†æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®æ²»ç†ä½“ç³»æ¶æ„                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      æ•°æ®æ ‡å‡†å±‚                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ å‘½åè§„èŒƒâ”‚ â”‚ æ•°æ®å­—å…¸â”‚ â”‚ æ•°æ®ç›®å½•â”‚ â”‚ è´¨é‡æ ‡å‡†â”‚ â”‚ å®‰å…¨ç­–ç•¥â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      å…ƒæ•°æ®ç®¡ç†å±‚                                â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚   â”‚
â”‚  â”‚  â”‚   æŠ€æœ¯å…ƒæ•°æ®    â”‚    â”‚   ä¸šåŠ¡å…ƒæ•°æ®    â”‚                     â”‚   â”‚
â”‚  â”‚  â”‚  - è¡¨ç»“æ„       â”‚    â”‚  - ä¸šåŠ¡å®šä¹‰     â”‚                     â”‚   â”‚
â”‚  â”‚  â”‚  - æ•°æ®ç±»å‹     â”‚    â”‚  - æ•°æ®æ¥æº     â”‚                     â”‚   â”‚
â”‚  â”‚  â”‚  - è¡€ç¼˜å…³ç³»     â”‚    â”‚  - è´£ä»»äºº       â”‚                     â”‚   â”‚
â”‚  â”‚  â”‚  - å­˜å‚¨ä½ç½®     â”‚    â”‚  - æ•æ„Ÿåº¦       â”‚                     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  å…ƒæ•°æ®é‡‡é›†å·¥å…·: Apache Atlas / DataHub / Amundsen               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      æ•°æ®è´¨é‡å±‚                                  â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚              Great Expectations è´¨é‡æ¡†æ¶                 â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ å®Œæ•´æ€§æ£€æŸ¥  â”‚  â”‚ ä¸€è‡´æ€§æ£€æŸ¥  â”‚  â”‚ å‡†ç¡®æ€§æ£€æŸ¥  â”‚      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - éç©ºæ£€æŸ¥  â”‚  â”‚ - æ ¼å¼æ ¡éªŒ  â”‚  â”‚ - å€¼åŸŸæ£€æŸ¥  â”‚      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - å”¯ä¸€æ€§    â”‚  â”‚ - å…³è”æ ¡éªŒ  â”‚  â”‚ - ç»Ÿè®¡æ£€æŸ¥  â”‚      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ åŠæ—¶æ€§æ£€æŸ¥  â”‚  â”‚ æœ‰æ•ˆæ€§æ£€æŸ¥  â”‚  â”‚ è‡ªå®šä¹‰è§„åˆ™  â”‚      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - å»¶è¿Ÿç›‘æ§  â”‚  â”‚ - æšä¸¾å€¼    â”‚  â”‚ - SQLè§„åˆ™   â”‚      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - æ–°é²œåº¦    â”‚  â”‚ - æ­£åˆ™åŒ¹é…  â”‚  â”‚ - Python    â”‚      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      æ•°æ®å®‰å…¨å±‚                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ æƒé™æ§åˆ¶â”‚ â”‚ æ•°æ®è„±æ•â”‚ â”‚ åŠ å¯†ä¼ è¾“â”‚ â”‚ å®¡è®¡æ—¥å¿—â”‚ â”‚ åˆè§„æ£€æŸ¥â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Great Expectations æ•°æ®è´¨é‡æ¡†æ¶

**1. æ ¸å¿ƒæ¦‚å¿µ**

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **Expectation** | æ•°æ®æœŸæœ›/è§„åˆ™ï¼Œå¦‚ `expect_column_values_to_not_be_null` |
| **Expectation Suite** | ä¸€ç»„ Expectation çš„é›†åˆï¼Œé’ˆå¯¹ç‰¹å®šæ•°æ®é›† |
| **Checkpoint** | å®šæœŸè¿è¡Œçš„æ£€æŸ¥ç‚¹ï¼Œç”¨äºéªŒè¯æ•°æ® |
| **Validation Result** | éªŒè¯ç»“æœï¼ŒåŒ…å«é€šè¿‡/å¤±è´¥çŠ¶æ€ |
| **Data Docs** | è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®è´¨é‡æ–‡æ¡£ |

**2. Great Expectations ä»£ç ç¤ºä¾‹**

```python
import great_expectations as gx
from great_expectations.core.expectation_suite import ExpectationSuite
from great_expectations.expectations import (
    ExpectColumnValuesToNotBeNull,
    ExpectColumnValuesToBeBetween,
    ExpectColumnValuesToMatchRegex,
    ExpectColumnDistinctValuesToContainSet
)

# åˆå§‹åŒ– GX ä¸Šä¸‹æ–‡
context = gx.get_context()

# åˆ›å»ºæ•°æ®æº
datasource = context.sources.add_pandas("my_datasource")
data_asset = datasource.add_dataframe_asset(name="orders_asset")

# åˆ›å»º Expectation Suite
suite = ExpectationSuite(name="orders_suite")

# 1. éç©ºæ£€æŸ¥
suite.add_expectation(
    ExpectColumnValuesToNotBeNull(column="order_id")
)
suite.add_expectation(
    ExpectColumnValuesToNotBeNull(column="user_id")
)
suite.add_expectation(
    ExpectColumnValuesToNotBeNull(column="amount")
)

# 2. å€¼åŸŸæ£€æŸ¥
suite.add_expectation(
    ExpectColumnValuesToBeBetween(
        column="amount",
        min_value=0,
        max_value=100000
    )
)

# 3. æ ¼å¼æ ¡éªŒ
suite.add_expectation(
    ExpectColumnValuesToMatchRegex(
        column="email",
        regex=r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
    )
)

# 4. æšä¸¾å€¼æ£€æŸ¥
suite.add_expectation(
    ExpectColumnDistinctValuesToContainSet(
        column="order_status",
        value_set=["pending", "paid", "shipped", "delivered", "cancelled"]
    )
)

# ä¿å­˜ Suite
context.save_expectation_suite(suite)

# åˆ›å»º Checkpoint
checkpoint = context.add_checkpoint(
    name="orders_checkpoint",
    validations=[
        {
            "batch_request": {
                "datasource_name": "my_datasource",
                "data_asset_name": "orders_asset",
            },
            "expectation_suite_name": "orders_suite",
        }
    ],
    action_list=[
        {
            "name": "store_validation_result",
            "action": {"class_name": "StoreValidationResultAction"}
        },
        {
            "name": "update_data_docs",
            "action": {"class_name": "UpdateDataDocsAction"}
        },
        {
            "name": "send_slack_notification",
            "action": {
                "class_name": "SlackNotificationAction",
                "slack_webhook": "${SLACK_WEBHOOK_URL}",
                "notify_on": "failure",
                "renderer": {
                    "module_name": "great_expectations.render.renderer.slack_renderer",
                    "class_name": "SlackRenderer"
                }
            }
        }
    ]
)

# è¿è¡Œ Checkpoint
result = checkpoint.run()
```

**3. é«˜çº§ç”¨æ³•ï¼šè‡ªå®šä¹‰ Expectation**

```python
from great_expectations.expectations import BatchExpectation
from great_expectations.core import ExpectationValidationResult
from great_expectations.execution_engine import PandasExecutionEngine
from great_expectations.expectations.metrics import MetricConfiguration

class ExpectTableRowCountToBeWithinRange(BatchExpectation):
    """è‡ªå®šä¹‰æœŸæœ›ï¼šè¡¨è¡Œæ•°åœ¨æŒ‡å®šèŒƒå›´å†…"""
    
    metric_dependencies = ("table.row_count",)
    
    success_keys = ("min_value", "max_value")
    
    default_kwarg_values = {
        "min_value": 0,
        "max_value": None,
    }
    
    def _validate(
        self,
        configuration,
        metrics,
        runtime_configuration=None,
        execution_engine=None,
    ):
        row_count = metrics.get("table.row_count")
        min_value = self.get_success_kwargs(configuration).get("min_value")
        max_value = self.get_success_kwargs(configuration).get("max_value")
        
        success = True
        if min_value is not None:
            success = success and (row_count >= min_value)
        if max_value is not None:
            success = success and (row_count <= max_value)
        
        return ExpectationValidationResult(
            success=success,
            result={"observed_value": row_count},
        )

# ä½¿ç”¨è‡ªå®šä¹‰æœŸæœ›
suite.add_expectation(
    ExpectTableRowCountToBeWithinRange(
        min_value=1000,
        max_value=10000000
    )
)
```

**4. é›†æˆåˆ°æ•°æ®æµæ°´çº¿**

```python
# Airflow DAG é›†æˆç¤ºä¾‹
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email': ['data-team@company.com'],
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'data_quality_pipeline',
    default_args=default_args,
    description='Data Quality Pipeline with Great Expectations',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
) as dag:
    
    # 1. æ•°æ®æŠ½å–
    extract_data = PostgresOperator(
        task_id='extract_data',
        postgres_conn_id='source_db',
        sql='SELECT * FROM orders WHERE dt = {{ ds }}',
    )
    
    # 2. æ•°æ®è´¨é‡æ£€æŸ¥
    validate_data = GreatExpectationsOperator(
        task_id='validate_data',
        data_context_root_dir='/opt/airflow/gx',
        checkpoint_name='orders_checkpoint',
        fail_task_on_validation_failure=True,
    )
    
    # 3. æ¡ä»¶åˆ†æ”¯ï¼šè´¨é‡æ£€æŸ¥é€šè¿‡æ‰åŠ è½½
    def check_validation_result(**context):
        result = context['ti'].xcom_pull(task_ids='validate_data')
        if not result['success']:
            raise ValueError("Data validation failed!")
        return True
    
    check_quality = PythonOperator(
        task_id='check_quality',
        python_callable=check_validation_result,
        provide_context=True,
    )
    
    # 4. æ•°æ®åŠ è½½
    load_data = PostgresOperator(
        task_id='load_data',
        postgres_conn_id='target_db',
        sql='INSERT INTO dw.orders SELECT * FROM staging.orders',
    )
    
    # ä¾èµ–å…³ç³»
    extract_data >> validate_data >> check_quality >> load_data
```

### 3.3 æ•°æ®è´¨é‡ç›‘æ§çœ‹æ¿

```yaml
# æ•°æ®è´¨é‡è§„åˆ™é…ç½®ç¤ºä¾‹ (YAML)
data_quality_rules:
  - table: orders
    columns:
      - name: order_id
        rules:
          - type: not_null
            severity: critical
          - type: unique
            severity: critical
          
      - name: amount
        rules:
          - type: not_null
            severity: critical
          - type: range
            min: 0
            max: 1000000
            severity: warning
          
      - name: order_status
        rules:
          - type: enum
            values: [pending, paid, shipped, delivered, cancelled]
            severity: warning
          
      - name: created_at
        rules:
          - type: not_null
            severity: critical
          - type: freshness
            max_delay: 24h
            severity: warning
    
  - table: users
    columns:
      - name: user_id
        rules:
          - type: not_null
            severity: critical
          - type: unique
            severity: critical
          
      - name: email
        rules:
          - type: regex
            pattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            severity: warning
          - type: not_null
            severity: warning

# å‘Šè­¦é…ç½®
alerts:
  slack:
    webhook_url: "${SLACK_WEBHOOK_URL}"
    channel: "#data-quality"
  
  email:
    recipients:
      - "data-team@company.com"
    smtp_server: "smtp.company.com"
  
  pagerduty:
    service_key: "${PAGERDUTY_KEY}"
    severity_levels: [critical]
```

---

## 4. å®æ—¶ç‰¹å¾å¹³å°æ¶æ„

### 4.1 ç‰¹å¾å¹³å°æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å®æ—¶ç‰¹å¾å¹³å° (Feature Platform)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     æ•°æ®æºå±‚ (Data Sources)                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ Kafka   â”‚ â”‚ MySQL   â”‚ â”‚  API    â”‚ â”‚  Log    â”‚ â”‚  File   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ (Event) â”‚ â”‚  (CDC)  â”‚ â”‚  (RPC)  â”‚ â”‚ (File)  â”‚ â”‚ (Batch) â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚           â”‚           â”‚           â”‚           â”‚             â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   ç‰¹å¾è®¡ç®—å±‚ (Feature Compute)                    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚    æµå¼ç‰¹å¾è®¡ç®—         â”‚    â”‚    æ‰¹å¼ç‰¹å¾è®¡ç®—         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚    (Flink/Spark)        â”‚    â”‚    (Spark/SQL)          â”‚     â”‚   â”‚
â”‚  â”‚  â”‚                         â”‚    â”‚                         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  - æ»‘åŠ¨çª—å£ç»Ÿè®¡         â”‚    â”‚  - å†å²èšåˆç‰¹å¾         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  - ä¼šè¯ç‰¹å¾             â”‚    â”‚  - ç”¨æˆ·ç”»åƒæ ‡ç­¾         â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  - å®æ—¶CTRç‰¹å¾          â”‚    â”‚  - ç›¸ä¼¼åº¦è®¡ç®—           â”‚     â”‚   â”‚
â”‚  â”‚  â”‚  - å¼‚å¸¸æ£€æµ‹             â”‚    â”‚  - å›¾ç‰¹å¾               â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â”‚             â”‚                              â”‚                     â”‚   â”‚
â”‚  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚   â”‚
â”‚  â”‚                            â–¼                                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚              Feature Store (ç‰¹å¾å­˜å‚¨)                    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚           Online Store (åœ¨çº¿ç‰¹å¾åº“)              â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - Redis / Tair / Aerospike                     â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - ä½å»¶è¿Ÿ (<10ms)                               â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - å®æ—¶ç‰¹å¾æœåŠ¡                                  â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                          â”‚                               â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                          â–¼                               â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚           Offline Store (ç¦»çº¿ç‰¹å¾åº“)             â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - Hive / Iceberg / ClickHouse                  â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - æ‰¹é‡è®­ç»ƒå’Œç¦»çº¿åˆ†æ                            â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  - ç‰¹å¾å¤ç”¨å’Œå…±äº«                                â”‚    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ç‰¹å¾æœåŠ¡å±‚ (Feature Service)                   â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  Feature Server (REST/gRPC API)                         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  Endpoints:                                              â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - GET /features/{entity_type}/{entity_id}               â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - POST /features/batch (æ‰¹é‡è·å–)                       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - POST /features/join (ç‰¹å¾æ‹¼æ¥)                        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - GET /features/stats (ç‰¹å¾ç»Ÿè®¡)                        â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  Feature Registry (ç‰¹å¾æ³¨å†Œä¸­å¿ƒ)                         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ç‰¹å¾å®šä¹‰å’Œå…ƒæ•°æ®                                      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ç‰ˆæœ¬æ§åˆ¶                                             â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - è¡€ç¼˜è¿½è¸ª                                             â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    æ¶ˆè´¹å±‚ (Consumers)                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ æ¨èç³»ç»Ÿâ”‚ â”‚ é£æ§æ¨¡å‹â”‚ â”‚ å¹¿å‘Šç³»ç»Ÿâ”‚ â”‚ æœç´¢æ’åºâ”‚ â”‚ è¿è¥åˆ†æâ”‚   â”‚   â”‚
â”‚  â”‚  â”‚         â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 å®æ—¶ç‰¹å¾è®¡ç®—ä»£ç ç¤ºä¾‹

**1. Flink å®æ—¶ç‰¹å¾è®¡ç®—**

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;

public class RealtimeFeatureEngineering {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        
        // è¯»å–ç‚¹å‡»æµæ•°æ®
        tableEnv.executeSql("""
            CREATE TABLE click_events (
                user_id STRING,
                item_id STRING,
                category_id STRING,
                action STRING,
                event_time TIMESTAMP(3),
                WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
            ) WITH (
                'connector' = 'kafka',
                'topic' = 'click_events',
                'properties.bootstrap.servers' = 'kafka:9092',
                'format' = 'json'
            )
        """);
        
        // 1. ç”¨æˆ·å®æ—¶è¡Œä¸ºç‰¹å¾ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
        tableEnv.executeSql("""
            CREATE TABLE user_behavior_features (
                user_id STRING PRIMARY KEY NOT ENFORCED,
                window_start TIMESTAMP(3),
                click_count_5m BIGINT,
                click_count_1h BIGINT,
                unique_items_5m BIGINT,
                category_diversity INT
            ) WITH (
                'connector' = 'jdbc',
                'url' = 'jdbc:mysql://feature-store:3306/features',
                'table-name' = 'user_behavior_features',
                'username' = 'root',
                'password' = 'password'
            )
        """);
        
        // æ»‘åŠ¨çª—å£èšåˆè®¡ç®—ç”¨æˆ·è¡Œä¸ºç‰¹å¾
        tableEnv.executeSql("""
            INSERT INTO user_behavior_features
            SELECT
                user_id,
                HOP_START(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) as window_start,
                COUNT(*) as click_count_5m,
                COUNT(*) OVER (
                    PARTITION BY user_id
                    ORDER BY event_time
                    RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW
                ) as click_count_1h,
                COUNT(DISTINCT item_id) as unique_items_5m,
                COUNT(DISTINCT category_id) as category_diversity
            FROM click_events
            WHERE action = 'click'
            GROUP BY
                user_id,
                HOP(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE)
        """);
        
        // 2. ç‰©å“å®æ—¶çƒ­åº¦ç‰¹å¾
        tableEnv.executeSql("""
            CREATE TABLE item_popularity_features (
                item_id STRING PRIMARY KEY NOT ENFORCED,
                window_time TIMESTAMP(3),
                click_count_10m BIGINT,
                ctr_10m DOUBLE,
                trend_score DOUBLE
            ) WITH (
                'connector' = 'redis',
                'host' = 'redis',
                'port' = '6379',
                'command' = 'SET'
            )
        """);
        
        // ç‰©å“çƒ­åº¦è®¡ç®—
        tableEnv.executeSql("""
            INSERT INTO item_popularity_features
            SELECT
                item_id,
                TUMBLE_END(event_time, INTERVAL '10' MINUTE) as window_time,
                COUNT(CASE WHEN action = 'click' THEN 1 END) as click_count_10m,
                CAST(COUNT(CASE WHEN action = 'click' THEN 1 END) AS DOUBLE) /
                    NULLIF(COUNT(CASE WHEN action = 'impression' THEN 1 END), 0) as ctr_10m,
                -- è¶‹åŠ¿åˆ†ï¼šå½“å‰çª—å£ç›¸å¯¹äºå‰ä¸€å‘¨æœŸçš„å¢é•¿ç‡
                (COUNT(CASE WHEN action = 'click' THEN 1 END) - 
                 LAG(COUNT(CASE WHEN action = 'click' THEN 1 END)) OVER (
                     PARTITION BY item_id ORDER BY TUMBLE_END(event_time, INTERVAL '10' MINUTE)
                 )) / NULLIF(LAG(COUNT(CASE WHEN action = 'click' THEN 1 END)) OVER (
                     PARTITION BY item_id ORDER BY TUMBLE_END(event_time, INTERVAL '10' MINUTE)
                 ), 0) as trend_score
            FROM click_events
            GROUP BY
                item_id,
                TUMBLE(event_time, INTERVAL '10' MINUTE)
        """);
        
        env.execute("Realtime Feature Engineering");
    }
}
```

**2. ç‰¹å¾æœåŠ¡ API (Python/Feast)**

```python
from feast import FeatureStore, Entity, Feature, FeatureView, ValueType
from feast.types import Float32, Int64, String
from datetime import timedelta
import pandas as pd

# åˆå§‹åŒ– Feature Store
store = FeatureStore(repo_path=".")

# å®šä¹‰å®ä½“
user = Entity(
    name="user_id",
    value_type=ValueType.STRING,
    description="ç”¨æˆ·ID",
)

item = Entity(
    name="item_id",
    value_type=ValueType.STRING,
    description="å•†å“ID",
)

# å®šä¹‰ç‰¹å¾è§†å›¾ï¼ˆåœ¨çº¿+ç¦»çº¿ï¼‰
user_behavior_fv = FeatureView(
    name="user_behavior_features",
    entities=["user_id"],
    ttl=timedelta(hours=2),
    features=[
        Feature(name="click_count_5m", dtype=Int64),
        Feature(name="click_count_1h", dtype=Int64),
        Feature(name="unique_items_5m", dtype=Int64),
        Feature(name="category_diversity", dtype=Int64),
    ],
    online=True,
    source=user_behavior_source,
)

item_popularity_fv = FeatureView(
    name="item_popularity_features",
    entities=["item_id"],
    ttl=timedelta(hours=1),
    features=[
        Feature(name="click_count_10m", dtype=Int64),
        Feature(name="ctr_10m", dtype=Float32),
        Feature(name="trend_score", dtype=Float32),
    ],
    online=True,
    source=item_popularity_source,
)

# ç‰¹å¾æœåŠ¡ API
class FeatureService:
    def __init__(self):
        self.store = FeatureStore(repo_path=".")
    
    def get_online_features(self, entity_rows: list, feature_refs: list):
        """è·å–åœ¨çº¿ç‰¹å¾ï¼ˆä½å»¶è¿Ÿï¼‰"""
        features = self.store.get_online_features(
            features=feature_refs,
            entity_rows=entity_rows,
        ).to_dict()
        return features
    
    def get_historical_features(self, entity_df: pd.DataFrame, feature_refs: list):
        """è·å–å†å²ç‰¹å¾ï¼ˆç”¨äºè®­ç»ƒï¼‰"""
        features = self.store.get_historical_features(
            entity_df=entity_df,
            features=feature_refs,
        ).to_df()
        return features
    
    def batch_get_features(self, entity_type: str, entity_ids: list):
        """æ‰¹é‡è·å–ç‰¹å¾"""
        entity_rows = [{entity_type: id} for id in entity_ids]
        
        if entity_type == "user_id":
            feature_refs = [
                "user_behavior_features:click_count_5m",
                "user_behavior_features:click_count_1h",
                "user_behavior_features:unique_items_5m",
            ]
        else:
            feature_refs = [
                "item_popularity_features:click_count_10m",
                "item_popularity_features:ctr_10m",
            ]
        
        return self.get_online_features(entity_rows, feature_refs)

# FastAPI æœåŠ¡
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()
feature_service = FeatureService()

class FeatureRequest(BaseModel):
    user_id: str
    item_ids: list[str]

@app.post("/features/realtime")
async def get_realtime_features(request: FeatureRequest):
    # è·å–ç”¨æˆ·ç‰¹å¾
    user_features = feature_service.get_online_features(
        entity_rows=[{"user_id": request.user_id}],
        feature_refs=[
            "user_behavior_features:click_count_5m",
            "user_behavior_features:category_diversity",
        ]
    )
    
    # è·å–ç‰©å“ç‰¹å¾
    item_features = feature_service.batch_get_features(
        entity_type="item_id",
        entity_ids=request.item_ids
    )
    
    return {
        "user_features": user_features,
        "item_features": item_features,
        "timestamp": pd.Timestamp.now().isoformat()
    }

@app.get("/features/stats/{feature_name}")
async def get_feature_stats(feature_name: str):
    """è·å–ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯"""
    # è¿”å›ç‰¹å¾çš„åˆ†å¸ƒã€ç¼ºå¤±ç‡ã€ç»Ÿè®¡å€¼ç­‰
    return {
        "feature_name": feature_name,
        "mean": 0.5,
        "std": 0.2,
        "missing_rate": 0.01,
        "last_updated": "2024-01-15T10:00:00Z"
    }
```

**3. ç‰¹å¾ç›‘æ§ä¸è¡€ç¼˜**

```python
# ç‰¹å¾æ¼‚ç§»ç›‘æ§
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

class FeatureMonitor:
    def __init__(self):
        self.drift_report = Report(metrics=[DataDriftPreset()])
    
    def check_drift(self, reference_data: pd.DataFrame, current_data: pd.DataFrame):
        """æ£€æµ‹ç‰¹å¾æ¼‚ç§»"""
        self.drift_report.run(
            reference_data=reference_data,
            current_data=current_data,
            column_mapping=ColumnMapping()
        )
        
        drift_score = self.drift_report.as_dict()['metrics'][0]['result']['dataset_drift']
        
        if drift_score:
            # å‘é€å‘Šè­¦
            self.send_alert(f"Feature drift detected! Score: {drift_score}")
        
        return drift_score
    
    def track_feature_lineage(self, feature_name: str, source_tables: list, 
                              transform_logic: str):
        """è¿½è¸ªç‰¹å¾è¡€ç¼˜"""
        lineage = {
            "feature_name": feature_name,
            "source_tables": source_tables,
            "transform": transform_logic,
            "created_at": pd.Timestamp.now(),
            "owner": "data-science-team"
        }
        # å­˜å‚¨åˆ°å…ƒæ•°æ®ç³»ç»Ÿ
        self.store_lineage(lineage)
        return lineage
```

---

## 5. æ•°æ®æµæ°´çº¿ç¼–æ’

### 5.1 Dagster vs Airflow å¯¹æ¯”

| ç»´åº¦ | Apache Airflow | Dagster |
|------|----------------|---------|
| **å‘å¸ƒæ—¶é—´** | 2015 | 2018 |
| **è®¾è®¡ç†å¿µ** | Task-centric | Asset-centric |
| **æ•°æ®æ„ŸçŸ¥** | âŒ æ—  | âœ… åŸç”Ÿæ”¯æŒ |
| **ç±»å‹å®‰å…¨** | âŒ å¼±ç±»å‹ | âœ… å¼ºç±»å‹ (Python) |
| **æµ‹è¯•æ”¯æŒ** | æœ‰é™ | ä¼˜ç§€ |
| **è°ƒè¯•ä½“éªŒ** | ä¸€èˆ¬ | ä¼˜ç§€ |
| **UI/UX** | ä¼ ç»Ÿ | ç°ä»£åŒ– |
| **å­¦ä¹ æ›²çº¿** | ä¸­ç­‰ | ç¨é™¡ |
| **ç¤¾åŒºè§„æ¨¡** | â­â­â­â­â­ | â­â­â­ |
| **é›†æˆç”Ÿæ€** | ä¸°å¯Œ (500+ Operators) | å¢é•¿ä¸­ |
| **äº‘åŸç”Ÿ** | æ”¯æŒ | åŸç”Ÿæ”¯æŒ |
| **é€‚ç”¨åœºæ™¯** | ä¼ ç»ŸETL | æ•°æ®èµ„äº§åŒ– |

### 5.2 Dagster ç°ä»£æµæ°´çº¿

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Dagster æ¶æ„å›¾                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      Dagster Webserver                           â”‚   â”‚
â”‚  â”‚  - å¯è§†åŒ–ç¼–æ’ç•Œé¢                                                â”‚   â”‚
â”‚  â”‚  - èµ„äº§è¡€ç¼˜å›¾                                                    â”‚   â”‚
â”‚  â”‚  - æ‰§è¡Œç›‘æ§                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      Dagster Daemon                              â”‚   â”‚
â”‚  â”‚  - è°ƒåº¦å™¨ (Scheduler)                                           â”‚   â”‚
â”‚  â”‚  - ä¼ æ„Ÿå™¨ (Sensors)                                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Dagster Code Location                         â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚                    Assets (æ•°æ®èµ„äº§)                     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â”‚ raw_    â”‚â”€â”€â”€â”€â”€â–¶â”‚ clean_  â”‚â”€â”€â”€â”€â”€â–¶â”‚ metrics â”‚        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â”‚ events  â”‚      â”‚ events  â”‚      â”‚ table   â”‚        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚        â”‚                                    â”‚          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚        â–¼                                    â–¼          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â”‚ S3/     â”‚                           â”‚ Feature â”‚    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â”‚ Iceberg â”‚                           â”‚ Store   â”‚    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚                   Jobs (ä½œä¸šç¼–æ’)                        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   @job                                                     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   def daily_etl():                                         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚       extract_data()                                       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚       transform_data()                                     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚       load_to_warehouse()                                  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚                Resources (èµ„æºé…ç½®)                      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   - Database connections                                 â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   - Spark sessions                                       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚   - Cloud storage clients                                â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Dagster ä»£ç ç¤ºä¾‹

```python
# dagster_pipeline.py
import dagster as dg
from dagster import asset, job, op, resource, Definitions
from dagster_duckdb import DuckDBResource
import pandas as pd

# 1. å®šä¹‰èµ„æº
@resource
def s3_resource():
    import boto3
    return boto3.client('s3')

@resource
def postgres_resource():
    import psycopg2
    return psycopg2.connect(
        host="localhost",
        database="warehouse",
        user="admin",
        password="password"
    )

# 2. å®šä¹‰æ•°æ®èµ„äº§ (Asset-centric)
@asset(
    group_name="raw_data",
    description="ä»S3åŠ è½½åŸå§‹è®¢å•æ•°æ®",
    metadata={"source": "s3://bucket/raw_orders/"}
)
def raw_orders(s3: dg.ResourceParam) -> pd.DataFrame:
    """ä» S3 åŠ è½½åŸå§‹è®¢å•æ•°æ®"""
    # ä» S3 è¯»å–æ•°æ®
    obj = s3.get_object(
        Bucket='data-lake',
        Key='raw/orders/orders.parquet'
    )
    return pd.read_parquet(obj['Body'])

@asset(
    group_name="staging",
    description="æ¸…æ´—åçš„è®¢å•æ•°æ®",
    deps=[raw_orders]
)
def stg_orders(raw_orders: pd.DataFrame) -> pd.DataFrame:
    """æ¸…æ´—è®¢å•æ•°æ®"""
    df = raw_orders.copy()
    
    # æ•°æ®æ¸…æ´—
    df = df.dropna(subset=['order_id', 'user_id'])
    df = df[df['amount'] > 0]
    df['order_date'] = pd.to_datetime(df['order_date'])
    
    # æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆGreat Expectations é›†æˆï¼‰
    assert df['order_id'].is_unique, "order_id å¿…é¡»å”¯ä¸€"
    assert df['amount'].min() > 0, "amount å¿…é¡»å¤§äº0"
    
    return df

@asset(
    group_name="analytics",
    description="æ¯æ—¥è®¢å•ç»Ÿè®¡",
    deps=[stg_orders]
)
def daily_order_metrics(stg_orders: pd.DataFrame) -> pd.DataFrame:
    """è®¡ç®—æ¯æ—¥è®¢å•ç»Ÿè®¡"""
    metrics = stg_orders.groupby(
        stg_orders['order_date'].dt.date
    ).agg({
        'order_id': 'count',
        'amount': ['sum', 'mean'],
        'user_id': 'nunique'
    }).reset_index()
    
    metrics.columns = ['date', 'order_count', 'total_amount', 'avg_amount', 'unique_users']
    return metrics

@asset(
    group_name="analytics",
    description="ç”¨æˆ·RFMåˆ†æ",
    deps=[stg_orders]
)
def rfm_analysis(stg_orders: pd.DataFrame) -> pd.DataFrame:
    """RFM ç”¨æˆ·ä»·å€¼åˆ†æ"""
    from datetime import datetime
    
    reference_date = stg_orders['order_date'].max()
    
    rfm = stg_orders.groupby('user_id').agg({
        'order_date': lambda x: (reference_date - x.max()).days,  # Recency
        'order_id': 'count',  # Frequency
        'amount': 'sum'  # Monetary
    }).reset_index()
    
    rfm.columns = ['user_id', 'recency', 'frequency', 'monetary']
    
    # RFM è¯„åˆ†
    rfm['r_score'] = pd.qcut(rfm['recency'], 5, labels=[5,4,3,2,1])
    rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
    rfm['m_score'] = pd.qcut(rfm['monetary'], 5, labels=[1,2,3,4,5])
    
    rfm['rfm_score'] = rfm['r_score'].astype(str) + \
                       rfm['f_score'].astype(str) + \
                       rfm['m_score'].astype(str)
    
    return rfm

# 3. å®šä¹‰ä½œä¸š (Job)
@job
def daily_etl_job():
    """æ¯æ—¥ ETL ä½œä¸š"""
    # èµ„äº§ä¼šè‡ªåŠ¨æ ¹æ®ä¾èµ–å…³ç³»æ‰§è¡Œ
    pass

@job
def data_quality_check():
    """æ•°æ®è´¨é‡æ£€æŸ¥ä½œä¸š"""
    # å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„è´¨é‡æ£€æŸ¥æ­¥éª¤
    pass

# 4. å®šä¹‰è°ƒåº¦
from dagster import ScheduleDefinition

daily_schedule = ScheduleDefinition(
    job=daily_etl_job,
    cron_schedule="0 2 * * *",  # æ¯å¤©å‡Œæ™¨2ç‚¹
    name="daily_etl_schedule",
    execution_timezone="Asia/Shanghai"
)

# 5. å®šä¹‰ä¼ æ„Ÿå™¨ (Sensor)
from dagster import sensor, RunRequest

@sensor(job=daily_etl_job)
def s3_file_sensor():
    """ç›‘å¬ S3 æ–°æ–‡ä»¶åˆ°è¾¾"""
    # æ£€æŸ¥ S3 æ˜¯å¦æœ‰æ–°æ–‡ä»¶
    has_new_files = check_s3_for_new_files()
    
    if has_new_files:
        yield RunRequest(
            run_key="s3_new_files",
            run_config={}
        )

# 6. é…ç½®å®šä¹‰
defs = Definitions(
    assets=[raw_orders, stg_orders, daily_order_metrics, rfm_analysis],
    jobs=[daily_etl_job, data_quality_check],
    schedules=[daily_schedule],
    sensors=[s3_file_sensor],
    resources={
        "s3": s3_resource,
        "postgres": postgres_resource,
        "duckdb": DuckDBResource(database="analytics.duckdb")
    }
)
```

### 5.3 Airflow ä¼ ç»Ÿæµæ°´çº¿

```python
# airflow_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import pandas as pd

# é»˜è®¤å‚æ•°
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': True,
    'email': ['data-team@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

# å®šä¹‰ DAG
with DAG(
    'data_warehouse_etl',
    default_args=default_args,
    description='æ•°æ®ä»“åº“ ETL æµæ°´çº¿',
    schedule_interval='0 2 * * *',  # æ¯å¤©å‡Œæ™¨2ç‚¹
    start_date=days_ago(1),
    catchup=False,
    tags=['etl', 'warehouse'],
    max_active_runs=1,
) as dag:
    
    # 1. æ•°æ®æŠ½å–
    def extract_from_s3(**context):
        """ä» S3 æŠ½å–æ•°æ®"""
        s3_hook = S3Hook(aws_conn_id='aws_default')
        execution_date = context['ds']
        
        # è¯»å– Parquet æ–‡ä»¶
        key = f'raw/orders/dt={execution_date}/orders.parquet'
        file_obj = s3_hook.get_key(key, bucket_name='data-lake')
        
        df = pd.read_parquet(file_obj.get()['Body'])
        
        # ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®ä¾›ä¸‹æ¸¸ä½¿ç”¨
        df.to_parquet(f'/tmp/orders_{execution_date}.parquet')
        
        return f"Extracted {len(df)} records"
    
    extract_task = PythonOperator(
        task_id='extract_from_s3',
        python_callable=extract_from_s3,
        provide_context=True,
    )
    
    # 2. æ•°æ®è½¬æ¢
    def transform_data(**context):
        """æ•°æ®æ¸…æ´—å’Œè½¬æ¢"""
        execution_date = context['ds']
        
        # è¯»å–æ•°æ®
        df = pd.read_parquet(f'/tmp/orders_{execution_date}.parquet')
        
        # æ¸…æ´—
        df = df.dropna(subset=['order_id', 'user_id'])
        df = df[df['amount'] > 0]
        df['order_date'] = pd.to_datetime(df['order_date'])
        df['dt'] = execution_date
        
        # ä¿å­˜
        df.to_parquet(f'/tmp/clean_orders_{execution_date}.parquet', index=False)
        
        return f"Transformed {len(df)} records"
    
    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        provide_context=True,
    )
    
    # 3. æ•°æ®è´¨é‡æ£€æŸ¥
    def data_quality_check(**context):
        """æ•°æ®è´¨é‡æ£€æŸ¥"""
        execution_date = context['ds']
        df = pd.read_parquet(f'/tmp/clean_orders_{execution_date}.parquet')
        
        checks = {
            'row_count': len(df) > 0,
            'unique_order_id': df['order_id'].is_unique,
            'positive_amount': (df['amount'] > 0).all(),
            'valid_dates': df['order_date'].notna().all(),
        }
        
        failed_checks = [k for k, v in checks.items() if not v]
        
        if failed_checks:
            raise ValueError(f"Data quality checks failed: {failed_checks}")
        
        return f"All {len(checks)} quality checks passed"
    
    quality_task = PythonOperator(
        task_id='data_quality_check',
        python_callable=data_quality_check,
        provide_context=True,
    )
    
    # 4. åŠ è½½åˆ°æ•°æ®ä»“åº“
    load_to_ods = PostgresOperator(
        task_id='load_to_ods',
        postgres_conn_id='warehouse_conn',
        sql="""
            COPY ods.orders FROM '/tmp/clean_orders_{{ ds }}.parquet' 
            WITH (FORMAT parquet);
        """
    )
    
    # 5. æ›´æ–°ç»´åº¦è¡¨
    update_dimensions = PostgresOperator(
        task_id='update_dimensions',
        postgres_conn_id='warehouse_conn',
        sql="""
            -- æ›´æ–°ç”¨æˆ·ç»´åº¦
            INSERT INTO dim.users (user_id, first_order_date, last_order_date)
            SELECT 
                user_id,
                MIN(order_date) as first_order_date,
                MAX(order_date) as last_order_date
            FROM ods.orders
            WHERE dt = '{{ ds }}'
            GROUP BY user_id
            ON CONFLICT (user_id) DO UPDATE SET
                last_order_date = EXCLUDED.last_order_date;
        """
    )
    
    # 6. æ›´æ–°äº‹å®è¡¨
    update_facts = PostgresOperator(
        task_id='update_facts',
        postgres_conn_id='warehouse_conn',
        sql="""
            -- æ¯æ—¥è®¢å•ç»Ÿè®¡
            INSERT INTO fct.daily_order_stats (dt, order_count, total_amount, unique_users)
            SELECT 
                dt,
                COUNT(*) as order_count,
                SUM(amount) as total_amount,
                COUNT(DISTINCT user_id) as unique_users
            FROM ods.orders
            WHERE dt = '{{ ds }}'
            GROUP BY dt;
        """
    )
    
    # 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    def cleanup(**context):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import os
        execution_date = context['ds']
        
        files = [
            f'/tmp/orders_{execution_date}.parquet',
            f'/tmp/clean_orders_{execution_date}.parquet'
        ]
        
        for f in files:
            if os.path.exists(f):
                os.remove(f)
        
        return "Cleanup completed"
    
    cleanup_task = PythonOperator(
        task_id='cleanup',
        python_callable=cleanup,
        provide_context=True,
        trigger_rule='all_done',  # æ— è®ºæˆåŠŸä¸å¦éƒ½æ‰§è¡Œ
    )
    
    # å®šä¹‰ä¾èµ–å…³ç³»
    extract_task >> transform_task >> quality_task >> load_to_ods
    load_to_ods >> [update_dimensions, update_facts]
    [update_dimensions, update_facts] >> cleanup_task
```

### 5.4 æµæ°´çº¿ç¼–æ’é€‰å‹å»ºè®®

| åœºæ™¯ | æ¨èå·¥å…· | ç†ç”± |
|------|----------|------|
| ä¼ ç»ŸETL/æ•°æ®è¿ç§» | **Airflow** | æˆç†Ÿç¨³å®šï¼ŒOperatorä¸°å¯Œ |
| æ•°æ®èµ„äº§åŒ–/æ•°æ®ç½‘æ ¼ | **Dagster** | Asset-centricï¼Œæ•°æ®æ„ŸçŸ¥ |
| ML Pipeline | **Dagster/Kubeflow** | æ›´å¥½çš„MLå®éªŒè¿½è¸ª |
| å®æ—¶æµå¤„ç† | **Flink SQL + Dagster** | æµæ‰¹ä¸€ä½“ |
| äº‘åŸç”Ÿéƒ¨ç½² | **Dagster/Prefect** | æ›´ç°ä»£çš„æ¶æ„ |
| å·²æœ‰Airflowç”Ÿæ€ | **Airflow 2.x** | å¹³æ»‘å‡çº§ï¼Œä¿æŒå…¼å®¹ |
| å¿«é€ŸåŸå‹å¼€å‘ | **Dagster** | æ›´å¥½çš„å¼€å‘ä½“éªŒ |

---

## 6. ç»¼åˆæ¶æ„é€‰å‹å»ºè®®

### 6.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç°ä»£æ•°æ®æ¹–ä»“æ¶æ„ (Lakehouse Architecture)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        æ•°æ®é‡‡é›†å±‚                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ Kafka   â”‚ â”‚ CDC     â”‚ â”‚ Log     â”‚ â”‚ API     â”‚ â”‚ File    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Connect â”‚ â”‚ Debeziumâ”‚ â”‚ Filebeatâ”‚ â”‚ Webhook â”‚ â”‚ Upload  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    å®æ—¶è®¡ç®—å±‚ (Flink)                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ æ•°æ®æ¸…æ´—    â”‚  â”‚ å®æ—¶å…³è”    â”‚  â”‚ ç‰¹å¾è®¡ç®—    â”‚             â”‚   â”‚
â”‚  â”‚  â”‚ ETL        â”‚  â”‚ Window Join â”‚  â”‚ Feature Eng â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚            â”‚                â”‚                â”‚                         â”‚
â”‚            â–¼                â–¼                â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   å­˜å‚¨å±‚ (Lakehouse)                             â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚              Iceberg / Delta Lake (æ•°æ®æ¹–)               â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ODS (åŸå§‹æ•°æ®)                                        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - DWD (æ˜ç»†æ•°æ®)                                        â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                          â”‚                                      â”‚   â”‚
â”‚  â”‚                          â–¼                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚              StarRocks / ClickHouse (OLAP)               â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - DWS (æ±‡æ€»æ•°æ®)                                        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ADS (åº”ç”¨æ•°æ®)                                        â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                          â”‚                                      â”‚   â”‚
â”‚  â”‚                          â–¼                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚              Redis (Feature Store Online)                â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - å®æ—¶ç‰¹å¾æœåŠ¡                                          â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   æ•°æ®æ²»ç†å±‚                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ DataHub     â”‚  â”‚ GE Quality  â”‚  â”‚ RBAC/åŠ å¯†   â”‚             â”‚   â”‚
â”‚  â”‚  â”‚ (å…ƒæ•°æ®)    â”‚  â”‚ (æ•°æ®è´¨é‡)  â”‚  â”‚ (æ•°æ®å®‰å…¨)  â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   ç¼–æ’è°ƒåº¦å±‚                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚  â”‚  â”‚    Dagster              â”‚    â”‚    Airflow (å¯é€‰)       â”‚     â”‚   â”‚
â”‚  â”‚  â”‚    - Assetç¼–æ’          â”‚    â”‚    - ä¼ ç»ŸETL            â”‚     â”‚   â”‚
â”‚  â”‚  â”‚    - æ•°æ®è¡€ç¼˜           â”‚    â”‚    - å®šæ—¶ä»»åŠ¡           â”‚     â”‚   â”‚
â”‚  â”‚  â”‚    - è´¨é‡é›†æˆ           â”‚    â”‚    - å‘Šè­¦é€šçŸ¥           â”‚     â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                     â”‚
â”‚                                   â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   åº”ç”¨æ¶ˆè´¹å±‚                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ BIæŠ¥è¡¨  â”‚ â”‚ æ•°æ®å¤§å±â”‚ â”‚ æ¨èç³»ç»Ÿâ”‚ â”‚ é£æ§æ¨¡å‹â”‚ â”‚ å³å¸­æŸ¥è¯¢â”‚   â”‚   â”‚
â”‚  â”‚  â”‚(Supersetâ”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚/FineBI) â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 æŠ€æœ¯æ ˆé€‰å‹çŸ©é˜µ

| å±‚çº§ | ç»„ä»¶A | ç»„ä»¶B | ç»„ä»¶C | æ¨èç»„åˆ |
|------|-------|-------|-------|----------|
| **å­˜å‚¨æ ¼å¼** | Iceberg | Delta Lake | Hudi | **Iceberg** (ç”Ÿæ€æ›´å¹¿) |
| **å®æ—¶è®¡ç®—** | Flink | Spark Streaming | Kafka Streams | **Flink** (æµå¤„ç†é¦–é€‰) |
| **OLAPå¼•æ“** | StarRocks | ClickHouse | Doris | **StarRocks** (æ˜“ç”¨æ€§å¥½) |
| **ç‰¹å¾å­˜å‚¨** | Redis | Tair | Aerospike | **Redis** (ç”Ÿæ€æˆç†Ÿ) |
| **æ•°æ®è´¨é‡** | GE | Deequ | Soda | **Great Expectations** |
| **å…ƒæ•°æ®** | DataHub | Atlas | Amundsen | **DataHub** (ç°ä»£åŒ–) |
| **ç¼–æ’è°ƒåº¦** | Dagster | Airflow 2.x | Prefect | **Dagster** (æ•°æ®ä¼˜å…ˆ) |

### 6.3 ä¸åŒè§„æ¨¡ä¼ä¸šé€‰å‹å»ºè®®

#### åˆåˆ›ä¼ä¸š (< 10äººæ•°æ®å›¢é˜Ÿ)
| å±‚çº§ | æ¨èæ–¹æ¡ˆ | é¢„ä¼°æœˆæˆæœ¬ |
|------|----------|------------|
| å­˜å‚¨ | S3 + Iceberg | $200-500 |
| è®¡ç®— | Flink on K8s | $300-800 |
| OLAP | StarRocks (è‡ªå»º) | $200-500 |
| ç¼–æ’ | Dagster (å¼€æº) | $0 |
| è´¨é‡ | Great Expectations | $0 |
| æ€»è®¡ | | $700-1800 |

#### ä¸­å‹ä¼ä¸š (10-50äººæ•°æ®å›¢é˜Ÿ)
| å±‚çº§ | æ¨èæ–¹æ¡ˆ | è¯´æ˜ |
|------|----------|------|
| å­˜å‚¨ | äº‘å‚å•†æ‰˜ç®¡Lakehouse | é˜¿é‡Œäº‘DLF/AW Glue |
| è®¡ç®— | Ververica/Flinkå…¨æ‰˜ç®¡ | é™ä½è¿ç»´æˆæœ¬ |
| OLAP | StarRocksä¼ä¸šç‰ˆ/äº‘æ‰˜ç®¡ | é«˜å¯ç”¨éƒ¨ç½² |
| ç‰¹å¾å¹³å° | Tair/äº‘Redisä¼ä¸šç‰ˆ | é«˜å¹¶å‘æ”¯æŒ |
| ç¼–æ’ | Dagster Cloud | æ‰˜ç®¡æœåŠ¡ |
| æ²»ç† | DataHub + GEä¼ä¸šç‰ˆ | å®Œæ•´æ–¹æ¡ˆ |

#### å¤§å‹ä¼ä¸š (> 50äººæ•°æ®å›¢é˜Ÿ)
| å±‚çº§ | æ¨èæ–¹æ¡ˆ | è¯´æ˜ |
|------|----------|------|
| å­˜å‚¨ | è‡ªç ”/å¼€æºLakehouse + å¤šäº‘ | é¿å…é”å®š |
| è®¡ç®— | Flink + Spark æ··åˆ | æµæ‰¹ä¸€ä½“ |
| OLAP | StarRocks/ClickHouse é›†ç¾¤ | ä¸¤åœ°ä¸‰ä¸­å¿ƒ |
| ç‰¹å¾å¹³å° | è‡ªç ”Feature Store | å®šåˆ¶åŒ–éœ€æ±‚ |
| ç¼–æ’ | Dagster + Airflow æ··åˆ | æ¸è¿›è¿ç§» |
| æ²»ç† | è‡ªç ”Data Meshå¹³å° | æ•°æ®æ°‘ä¸»åŒ– |

### 6.4 å®æ–½è·¯çº¿å›¾

```
Phase 1 (1-3ä¸ªæœˆ): åŸºç¡€è®¾æ–½å»ºè®¾
â”œâ”€â”€ æ­å»º Iceberg/Delta Lake æ•°æ®æ¹–
â”œâ”€â”€ éƒ¨ç½² Flink é›†ç¾¤
â”œâ”€â”€ éƒ¨ç½² StarRocks OLAPå¼•æ“
â””â”€â”€ å»ºç«‹åŸºç¡€æ•°æ®æ¥å…¥æµç¨‹

Phase 2 (3-6ä¸ªæœˆ): æ ¸å¿ƒèƒ½åŠ›å»ºè®¾
â”œâ”€â”€ å®æ—¶æ•°ä»“åˆ†å±‚å»ºè®¾ (ODS/DWD/DWS)
â”œâ”€â”€ éƒ¨ç½² Dagster ç¼–æ’å¹³å°
â”œâ”€â”€ é›†æˆ Great Expectations æ•°æ®è´¨é‡
â””â”€â”€ å»ºç«‹æ•°æ®è¡€ç¼˜è¿½è¸ª

Phase 3 (6-12ä¸ªæœˆ): é«˜çº§èƒ½åŠ›å®Œå–„
â”œâ”€â”€ å®æ—¶ç‰¹å¾å¹³å°å»ºè®¾
â”œâ”€â”€ æ•°æ®æ²»ç†ä½“ç³»è½åœ°
â”œâ”€â”€ æ•°æ®æœåŠ¡APIåŒ–
â””â”€â”€ æ•°æ®èµ„äº§ç›®å½•å‘å¸ƒ

Phase 4 (12ä¸ªæœˆ+): æ™ºèƒ½åŒ–å‡çº§
â”œâ”€â”€ AIé©±åŠ¨çš„æ•°æ®å‘ç°
â”œâ”€â”€ è‡ªåŠ¨æ•°æ®è´¨é‡ä¿®å¤
â”œâ”€â”€ æ™ºèƒ½è°ƒåº¦ä¼˜åŒ–
â””â”€â”€ æ•°æ®æˆæœ¬æ²»ç†
```

---

## é™„å½•

### A. å‚è€ƒèµ„æ–™

- [Apache Iceberg å®˜æ–¹æ–‡æ¡£](https://iceberg.apache.org/)
- [Delta Lake å®˜æ–¹æ–‡æ¡£](https://delta.io/)
- [Flink å®˜æ–¹æ–‡æ¡£](https://nightlies.apache.org/flink/)
- [StarRocks å®˜æ–¹æ–‡æ¡£](https://docs.starrocks.io/)
- [Great Expectations æ–‡æ¡£](https://docs.greatexpectations.io/)
- [Dagster æ–‡æ¡£](https://docs.dagster.io/)

### B. ä»£ç ä»“åº“ç»“æ„å»ºè®®

```
data-platform/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml          # æœ¬åœ°å¼€å‘ç¯å¢ƒ
â”œâ”€â”€ infra/                      # åŸºç¡€è®¾æ–½é…ç½®
â”‚   â”œâ”€â”€ terraform/              # äº‘èµ„æº
â”‚   â””â”€â”€ k8s/                    # K8s é…ç½®
â”œâ”€â”€ lakehouse/                  # Lakehouse é…ç½®
â”‚   â”œâ”€â”€ iceberg/
â”‚   â””â”€â”€ delta/
â”œâ”€â”€ streaming/                  # å®æ—¶è®¡ç®—
â”‚   â”œâ”€â”€ flink-jobs/
â”‚   â””â”€â”€ sql-scripts/
â”œâ”€â”€ warehouse/                  # æ•°ä»“æ¨¡å‹
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ feature-store/              # ç‰¹å¾å¹³å°
â”‚   â”œâ”€â”€ definitions/
â”‚   â””â”€â”€ pipelines/
â”œâ”€â”€ orchestration/              # ç¼–æ’é…ç½®
â”‚   â”œâ”€â”€ dagster/
â”‚   â””â”€â”€ airflow/
â”œâ”€â”€ quality/                    # æ•°æ®è´¨é‡
â”‚   â””â”€â”€ expectations/
â””â”€â”€ docs/                       # æ–‡æ¡£
```

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´: 2026-02-16*  
*ä½œè€…: Agent 3 - é›†ç¾¤å­¦ä¹ é¡¹ç›®*
