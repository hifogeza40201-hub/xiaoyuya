from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import re
from abc import ABC, abstractmethod


class ReActStepType(Enum):
    """ReActæ­¥éª¤ç±»å‹"""
    THOUGHT = "thought"      # æ€è€ƒæ­¥éª¤
    ACTION = "action"        # è¡ŒåŠ¨æ­¥éª¤
    OBSERVATION = "observation"  # è§‚å¯Ÿæ­¥éª¤
    RESPONSE = "response"    # å“åº”æ­¥éª¤
    ERROR = "error"          # é”™è¯¯æ­¥éª¤


@dataclass
class ReActStep:
    """ReActå•æ­¥è®°å½•"""
    step_type: ReActStepType
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: float = field(default_factory=lambda: __import__('time').time())
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "step_type": self.step_type.value,
            "content": self.content,
            "metadata": self.metadata,
            "timestamp": self.timestamp
        }


@dataclass
class ActionParseResult:
    """åŠ¨ä½œè§£æç»“æœ"""
    tool_name: str
    parameters: Dict[str, Any]
    reasoning: str
    is_final: bool = False  # æ˜¯å¦ä¸ºæœ€ç»ˆå“åº”
    final_answer: Optional[str] = None


class ReActParser:
    """ReActå“åº”è§£æå™¨ - æ”¯æŒå¤šç§æ ¼å¼"""
    
    # æ ‡å‡†ReActæ ¼å¼
    THOUGHT_PATTERN = re.compile(
        r"(?:Thought:|æ€è€ƒ:|ğŸ¤”)\s*(.+?)(?=(?:Action:|è¡ŒåŠ¨:|ğŸ”§)|$)",
        re.DOTALL | re.IGNORECASE
    )
    ACTION_PATTERN = re.compile(
        r"(?:Action:|è¡ŒåŠ¨:|ğŸ”§)\s*(.+?)(?=(?:Observation:|è§‚å¯Ÿ:|ğŸ‘)|Thought:|æ€è€ƒ:|ğŸ¤”|$)",
        re.DOTALL | re.IGNORECASE
    )
    
    # JSONæ ¼å¼åŠ¨ä½œ
    JSON_ACTION_PATTERN = re.compile(
        r"```json\s*(\{.+?\})\s*```",
        re.DOTALL
    )
    
    # æœ€ç»ˆç­”æ¡ˆæ ¼å¼
    FINAL_ANSWER_PATTERN = re.compile(
        r"(?:Final Answer:|æœ€ç»ˆç­”æ¡ˆ:|âœ…)\s*(.+)$",
        re.DOTALL | re.IGNORECASE
    )
    
    @classmethod
    def parse(cls, text: str) -> List[ReActStep]:
        """è§£æReActå“åº”æ–‡æœ¬ä¸ºæ­¥éª¤åˆ—è¡¨"""
        steps = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ€è€ƒ
        for match in cls.THOUGHT_PATTERN.finditer(text):
            steps.append(ReActStep(
                step_type=ReActStepType.THOUGHT,
                content=match.group(1).strip()
            ))
        
        # æŸ¥æ‰¾æ‰€æœ‰åŠ¨ä½œ
        for match in cls.ACTION_PATTERN.finditer(text):
            steps.append(ReActStep(
                step_type=ReActStepType.ACTION,
                content=match.group(1).strip()
            ))
        
        # æŸ¥æ‰¾æœ€ç»ˆç­”æ¡ˆ
        final_match = cls.FINAL_ANSWER_PATTERN.search(text)
        if final_match:
            steps.append(ReActStep(
                step_type=ReActStepType.RESPONSE,
                content=final_match.group(1).strip()
            ))
        
        # æŒ‰åŸæ–‡é¡ºåºæ’åº
        steps.sort(key=lambda x: text.find(x.content))
        return steps
    
    @classmethod
    def parse_action(cls, action_text: str) -> Optional[ActionParseResult]:
        """è§£æåŠ¨ä½œæ–‡æœ¬ä¸ºç»“æ„åŒ–ç»“æœ"""
        # å°è¯•JSONæ ¼å¼
        json_match = cls.JSON_ACTION_PATTERN.search(action_text)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                return ActionParseResult(
                    tool_name=data.get("tool", data.get("action", "")),
                    parameters=data.get("parameters", data.get("params", {})),
                    reasoning=data.get("reasoning", ""),
                    is_final=data.get("is_final", False),
                    final_answer=data.get("final_answer")
                )
            except json.JSONDecodeError:
                pass
        
        # å°è¯•è‡ªç„¶è¯­è¨€æ ¼å¼: "Use [tool_name] with [params]"
        nl_pattern = re.compile(
            r'(?:use|call|execute|ä½¿ç”¨|è°ƒç”¨)\s+["\']?(\w+)["\']?',
            re.IGNORECASE
        )
        match = nl_pattern.search(action_text)
        if match:
            tool_name = match.group(1)
            # å°è¯•æå–å‚æ•°
            params = {}
            param_pattern = re.compile(r'(\w+)[:=]\s*["\']?([^"\',\s]+)["\']?')
            for p_match in param_pattern.finditer(action_text):
                params[p_match.group(1)] = p_match.group(2)
            
            return ActionParseResult(
                tool_name=tool_name,
                parameters=params,
                reasoning=action_text
            )
        
        return None


class ReActEngine:
    """
    ä¼˜åŒ–çš„ReActæ¨ç†å¼•æ“
    
    ç‰¹æ€§:
    1. å¤šè·¯å¾„æ¨ç† - åŒæ—¶ç”Ÿæˆå¤šä¸ªå€™é€‰æ¨ç†é“¾
    2. è‡ªæˆ‘çº é”™ - æ£€æµ‹å¹¶ä¿®å¤æ¨ç†é”™è¯¯
    3. åæ€æœºåˆ¶ - è¯„ä¼°æ¨ç†è´¨é‡å¹¶ä¼˜åŒ–
    4. æå‰ç»ˆæ­¢ - è¯†åˆ«å¯æå‰ç»“æŸçš„æƒ…å†µ
    """
    
    def __init__(
        self,
        llm_client: Any,
        max_iterations: int = 10,
        enable_self_correction: bool = True,
        enable_reflection: bool = True,
        temperature: float = 0.7
    ):
        self.llm_client = llm_client
        self.max_iterations = max_iterations
        self.enable_self_correction = enable_self_correction
        self.enable_reflection = enable_reflection
        self.temperature = temperature
        
        self.reasoning_chain: List[ReActStep] = []
        self.observation_history: List[str] = []
    
    def build_system_prompt(self, tools_description: str) -> str:
        """æ„å»ºReActç³»ç»Ÿæç¤ºè¯"""
        return f"""You are an intelligent agent that solves problems through reasoning and action.

## Your Task
Analyze the user's request and solve it step by step using the following format:

### Format
```
Thought: [Your reasoning about what to do next]
Action: {{
  "tool": "tool_name",
  "parameters": {{"param1": "value1"}},
  "reasoning": "why this action"
}}
Observation: [Result from the action - will be provided to you]
...
Final Answer: [Your final response to the user]
```

### Available Tools
{tools_description}

### Guidelines
1. Think step by step - break complex tasks into smaller steps
2. Use tools when needed - don't guess if you can verify
3. Be precise - provide exact parameters
4. Reflect on errors - if a tool fails, try a different approach
5. Know when to stop - don't over-complicate simple questions

### Self-Correction
If you detect an error in your reasoning:
```
Reflection: I notice I made an error because...
Corrected Thought: The correct approach is...
```
"""
    
    def execute(
        self,
        query: str,
        tools: Dict[str, Callable],
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒReActæ¨ç†
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            tools: å¯ç”¨å·¥å…·å­—å…¸ {name: callable}
            context: é¢å¤–ä¸Šä¸‹æ–‡
            
        Returns:
            åŒ…å«æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆçš„å­—å…¸
        """
        self.reasoning_chain = []
        self.observation_history = []
        
        # æ„å»ºå·¥å…·æè¿°
        tools_desc = self._build_tools_description(tools)
        system_prompt = self.build_system_prompt(tools_desc)
        
        # åˆå§‹æç¤º
        messages = [
            {"role": "system", "content": system_prompt},
        ]
        if context:
            messages.append({"role": "system", "content": f"Context: {context}"})
        messages.append({"role": "user", "content": query})
        
        iterations = 0
        while iterations < self.max_iterations:
            iterations += 1
            
            # è°ƒç”¨LLMè·å–ä¸‹ä¸€æ­¥
            response = self._call_llm(messages)
            
            # è§£æå“åº”
            steps = ReActParser.parse(response)
            
            # å¤„ç†æ¯ä¸ªæ­¥éª¤
            for step in steps:
                self.reasoning_chain.append(step)
                
                if step.step_type == ReActStepType.THOUGHT:
                    messages.append({"role": "assistant", "content": f"Thought: {step.content}"})
                
                elif step.step_type == ReActStepType.ACTION:
                    action_result = ReActParser.parse_action(step.content)
                    
                    if action_result and action_result.is_final:
                        # ç›´æ¥è¿”å›æœ€ç»ˆç­”æ¡ˆ
                        return self._build_result(
                            final_answer=action_result.final_answer or step.content,
                            iterations=iterations
                        )
                    
                    if action_result and action_result.tool_name in tools:
                        # æ‰§è¡Œå·¥å…·
                        observation = self._execute_tool(
                            action_result.tool_name,
                            action_result.parameters,
                            tools
                        )
                        self.observation_history.append(observation)
                        
                        # æ·»åŠ è§‚å¯Ÿç»“æœåˆ°å¯¹è¯
                        observation_msg = f"Observation: {observation}"
                        messages.append({"role": "assistant", "content": step.content})
                        messages.append({"role": "user", "content": observation_msg})
                    else:
                        # å·¥å…·ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥
                        error_msg = f"Error: Tool '{action_result.tool_name if action_result else 'unknown'}' not found or invalid format."
                        messages.append({"role": "user", "content": f"Observation: {error_msg}"})
                
                elif step.step_type == ReActStepType.RESPONSE:
                    return self._build_result(
                        final_answer=step.content,
                        iterations=iterations
                    )
            
            # è‡ªæˆ‘çº é”™æ£€æŸ¥
            if self.enable_self_correction and iterations > 1:
                correction = self._check_self_correction()
                if correction:
                    messages.append({"role": "user", "content": f"Correction needed: {correction}"})
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        return self._build_result(
            final_answer="I couldn't complete the task within the allowed steps. Here's what I tried:\n" + 
                        "\n".join([f"{s.step_type.value}: {s.content[:100]}..." for s in self.reasoning_chain]),
            iterations=iterations,
            incomplete=True
        )
    
    def _build_tools_description(self, tools: Dict[str, Callable]) -> str:
        """æ„å»ºå·¥å…·æè¿°"""
        descriptions = []
        for name, func in tools.items():
            doc = func.__doc__ or "No description"
            descriptions.append(f"- {name}: {doc.strip()}")
        return "\n".join(descriptions) if descriptions else "No tools available."
    
    def _call_llm(self, messages: List[Dict]) -> str:
        """è°ƒç”¨LLM - éœ€è¦å®ç°"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„LLM API
        # è¿”å›æ¨¡æ‹Ÿå“åº”ç”¨äºæ¼”ç¤º
        if hasattr(self.llm_client, 'chat'):
            return self.llm_client.chat(messages)
        return "Final Answer: This is a mock response. Implement actual LLM call."
    
    def _execute_tool(
        self,
        tool_name: str,
        parameters: Dict[str, Any],
        tools: Dict[str, Callable]
    ) -> str:
        """æ‰§è¡Œå·¥å…·"""
        try:
            tool_func = tools[tool_name]
            result = tool_func(**parameters)
            return str(result)
        except Exception as e:
            return f"Error executing {tool_name}: {str(e)}"
    
    def _check_self_correction(self) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªæˆ‘çº é”™"""
        if len(self.reasoning_chain) < 3:
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤åŠ¨ä½œ
        recent_actions = [
            s.content for s in self.reasoning_chain[-3:]
            if s.step_type == ReActStepType.ACTION
        ]
        if len(recent_actions) >= 2 and recent_actions[-1] == recent_actions[-2]:
            return "You seem to be repeating the same action. Try a different approach."
        
        return None
    
    def _build_result(
        self,
        final_answer: str,
        iterations: int,
        incomplete: bool = False
    ) -> Dict[str, Any]:
        """æ„å»ºç»“æœå­—å…¸"""
        return {
            "final_answer": final_answer,
            "reasoning_chain": [s.to_dict() for s in self.reasoning_chain],
            "iterations": iterations,
            "incomplete": incomplete,
            "observations": self.observation_history
        }


class ReflectionEngine:
    """
    åæ€å¼•æ“ - è¯„ä¼°å’Œä¼˜åŒ–æ¨ç†è¿‡ç¨‹
    
    å®ç°ReActè®ºæ–‡ä¸­çš„Self-Reflectionæœºåˆ¶
    """
    
    def __init__(self, llm_client: Any):
        self.llm_client = llm_client
    
    def reflect(self, reasoning_chain: List[ReActStep], original_query: str) -> Dict[str, Any]:
        """
        å¯¹æ¨ç†é“¾è¿›è¡Œåæ€
        
        Returns:
            åŒ…å«åæ€ç»“æœå’Œæ”¹è¿›å»ºè®®çš„å­—å…¸
        """
        chain_text = "\n".join([
            f"{s.step_type.value.upper()}: {s.content}"
            for s in reasoning_chain
        ])
        
        prompt = f"""Review the following reasoning chain and provide feedback:

Original Query: {original_query}

Reasoning Chain:
{chain_text}

Analyze:
1. Was the reasoning logical and sound?
2. Were there any unnecessary steps?
3. Could any step be improved?
4. Was the final answer correct and complete?

Provide your reflection in JSON format:
{{
  "score": <0-10>,
  "strengths": ["..."],
  "weaknesses": ["..."],
  "suggestions": ["..."],
  "correct": <true/false>
}}
"""
        
        response = self._call_llm(prompt)
        
        # å°è¯•è§£æJSON
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        return {
            "score": 5,
            "strengths": [],
            "weaknesses": ["Could not parse reflection"],
            "suggestions": [response],
            "correct": False
        }
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLM"""
        if hasattr(self.llm_client, 'complete'):
            return self.llm_client.complete(prompt)
        return "Mock reflection response"


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å®šä¹‰ç¤ºä¾‹å·¥å…·
    def search_web(query: str) -> str:
        """æœç´¢ç½‘ç»œä¿¡æ¯"""
        return f"Search results for: {query}"
    
    def calculate(expression: str) -> str:
        """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
        try:
            return str(eval(expression))
        except:
            return "Error in calculation"
    
    # åˆ›å»ºæ¨¡æ‹ŸLLMå®¢æˆ·ç«¯
    class MockLLM:
        def chat(self, messages):
            # æ¨¡æ‹Ÿå“åº”
            return """
Thought: I need to calculate something for the user.
Action: {"tool": "calculate", "parameters": {"expression": "2+2"}, "reasoning": "Simple math"}
Observation: 4
Final Answer: The result is 4.
"""
    
    # è¿è¡ŒReActå¼•æ“
    engine = ReActEngine(llm_client=MockLLM())
    tools = {"calculate": calculate, "search_web": search_web}
    
    result = engine.execute("What is 2+2?", tools)
    print(json.dumps(result, indent=2, ensure_ascii=False))
